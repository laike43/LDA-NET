


 北方民族大学
 本科毕业论文（设计）

题目:    Python数据分析-文档话题检测                                      


院(系)名 称:       计算机科学与工程学院           
学 生 姓 名:               陈博                  
学       号:             20152243                
专       业:             网络工程                
指导教师姓名:             于千城         
论文提交时间:
    北方民族大学教务处制
Python数据分析-文档话题检测
摘要
随着互联网的普及,越来越多的人参与互联网并进行交流.电子邮件因其快速,低廉,丰富得到形式在互联网交流沟通中占有重要中作用,大量的用户构成一个人物复杂网络并产生大量的文本数据.研究这些邮件数据,可以从其中挖掘有价值的信息,如社会话题追踪,人物社交关系的构建等.
本论文一共分为五章,第一章是邮件文本主题检测课题的课题任务与背景介绍,国内外的研究现状及存在的问题,还有实现课题的技术路线.为了进一步学习文本主题提取和文本分类的理论,第二章主要是项目的技术原理及不同技术比较,如词干提取和词形还原,不同聚类算法的优劣及适用环境;有了理论指导,还要对邮件文本进行处理,第三章主要是文档话题检测预处理过程,主要是描述数据集特点,数据在数据库中存储格式,还有文本处理和词料向量转换,如分词,提取词干,统计词频等.经过处理过的词料,就能进一步用于检测话题与聚类,第四章主要是文档话题检测的实现和LDA模型产生的结果,并对结果进行k-means与birch聚类得到kmean聚类于birch聚类结果与分析.第五章主要是做项目时遇到的问题,及后续解决的方法.

关键词: Python,LDA,主题提取,聚类


Python Data Analysis-Document Topic Detection
Abstract
With the popularity of the Internet, more and more people participate in the Internet and communicate with each other. E-mail plays an important role in the Internet communication because of its fast, cheap and rich forms. A large number of users constitute a complex network of characters and generate a large amount of text data. The construction of relations, etc.
This paper is divided into five chapters. The first chapter is the introduction of the task and background of the subject, the current research situation and existing problems at home and abroad, and the technical route of realizing the subject. In order to further study the theory of text topic extraction and text classification, the second chapter is mainly about the technical principle of the project and the comparison of different technologies, such as stem extraction and morphology restoration, different clustering calculation. The third chapter is mainly about the pretreatment process of document topic detection, mainly describing the characteristics of data sets, data storage format in the database, text processing and corpus vector transformation, such as word segmentation, stem extraction, statistical word frequency, etc. The processed corpus can be further used for topic detection and clustering. Four chapters are mainly about the implementation of document topic detection and the results of LDA model, and the results are clustered by K-means and birch to get the results and analysis of kmean clustering in birch clustering. The fifth chapter is mainly about the problems encountered in the project and the subsequent solutions.

Keywords: Topic extraction , clustering


目录
第一章 绪论	1
1.1 课题任务与背景	1
1.2 国内外研究现状及存在的问题	2
1.3课题内容及技术路线	3
课题任务：	3
第二章技术原理	4
1.1 数据清洗	5
1.1.2分词处理	5
1.1.3  提取词干	6
2.2 聚类Kmeans Birch算法	8
2.2.1 birch算法介绍及特点	8
2.2.2 Kmeans 算法介绍及特点	9
2.3 Lda 算法介绍及特点	10
2.3.1  LDA文本原理及生成过程	11
第三章 文本预处理	13
3.1数据集概述	13
3.2 MySQL读取,存储	15
3.2.1 数据库读取	16
3.2.2 数据库结果保存	16
3.3 文本处理	17
3.3.1 NTLK介绍	17
3.3.2 分词提取词干	17
3.4 文本转换	18
4.1 gensim主题提取	20
4.1.1  LDA提取主题	20
图 4.2 词形还原处理	22
4.2 sklearn kmeans划分聚类	22
4.3 sklearn birch层次聚类	25
第五章：问题总结与展望	28
5.1 程序运行优化问题	28
图5.1 多线程图	28
5.2 聚类格式处理问题	29
5.3 聚类大维度问题	29
致谢	32
参考文献	33



第一章 绪论
1.1 课题任务与背景
本项目所要做的课题是利用Python编程语言实现LDA算法模型,对邮件数据集中每一封邮件进行处理后,并采用LDA文本话题发现方法检测邮件数据集中每一封邮件的内容所涉及的话题，对这些结果进行分析果并储存.挑选合适的聚类算法并用Python实现,对储存的话题分析结果进行聚类，实现按照话题类别对邮件进行分类.以此来追踪邮件话题并构建人物圈子关系.
在当今世界,电子邮件是互联网最广的电子信息交换服务.电子邮件以其低廉的价格,快速的服务,多形式的特点极大地方便了人与人之间的交流与沟通,并吸引大量的用户.不仅是个人用户,公司等单位也大量使用电子邮件作为主要的通信沟通工具.经过长时间的积累,大量的用户产生巨量的数据.研究这些邮件数据,可以从其中挖掘有价值的信息,如社会话题追踪,人物社交关系的构建等.
电子邮件因其特性不仅有互联网传统文本形式,还有产生许多长度较短,包含链接信息,新词汇,或简写词语的短文本.这些特征导致短文本有着不规则用语和文本特征稀疏的问题.如何选择并使用文本主题检测算法,来适应这种不同类别文本成为邮件检测重要问题. 安然公司（Enron）因其财务造假丑闻,管理者欺诈犯2002破产，人们收集了邮件数据.这份数据是真实网络环境数据,对这份数据集进行安然公司员工话题检测,关键人物关系分析,可以更好地观察安然公司经营问题,故数据集对文本数据分析有很强的实践意义.
主题模型作为一种文本内容的概率生成模型或产生式模型, 如潜在语义分析( LSA) 概率潜在语义分析(PLSA)和LDA, 通过对文章形成过程的模拟,找到产生文本的最佳主题和词汇,能够最大程度地表示文本中所蕴含的含义,信息丢失较少,较好地解决了词汇、主题和文本之间的语义关联问题,是目前最常用的文本主题提取方法[10]。
通过对安然公司邮件集分析检测.可以学习到LDA 主题模型在文本主题检测方面的应用。LDA 算法是到当前文本分析的一种主流算法，具有优秀的的算法理论支撑,LDA 是完全的生成模型，从理论上讲，具有其它模型无可比拟的建模优势[6]。除此之外还可以学习到NLTK自然语言处理对文本的预处理,如分句，分词等，以及sklearn在机器学习的聚类，特征提取，降维度方面的应用.当前在文本数据分析及机器学习中,它们扮演着很重要的角色.因此对邮件集处理分析,检测主题并分类对数据分析及机器学习有很强的学习意义.
1.2 国内外研究现状及存在的问题
在文本主题检测方面,国内外拥有许多算法。主要算法有潜在语义分析（LSA）,概率潜在语义分析(PLSA) LDA等算法,它们都在处理文本时具有某一方面优势.
潜在语义分析（LSA）和传统向量空间模型一样使用向量来表示词和文档,并通过向量间的关系(如夹角)来判断词及文档间的关系[2]。但LSA有不能无法解决词意义冲突问题。概率潜在语义分析(PLSA)类似于LSA的思想，但它也有自身的问题.如概率模型不够完备,在文档层面上没有提供合适的概率模型，使得PLSA并不是完备的生成式模型，而必须在确定文档的情况下才能对模型进行随机抽样，随着文档和词个数的增加，PLSA模型也线性增加，变得越来越庞大等等[3]。LDA即潜在狄利克雷分布，是PLSA 的贝叶斯版本。它使用狄利克雷先验来处理文档-主题和单词-主题分布，从而有助于更好地泛化[4]。
当前国内外有许多LDA算法的研究论文，大多是理论模型的研究和一些优化算法。这些理论模型与算法大都是针对某种类型文本。 如：微博内容短文本等。在实际应用中，因为工程数据一般较大，所以造成计算量大的特点。在考虑速率和文本检测效果后，大部分文本检测工程都会选用LSA ，LDA等知名算法以求效果和速率的平衡。因为工程上实现因计算量较大，一般由c++，java实现，
这些项目研究大多是爬取门户网站的数据，或微博这样的社交平台内容。一般实现的目的便是追踪社会热点话题，舆情舆论等。也就是说，大部分的项目是针对某种平台文本特点来进行分析，并且只对文本进行进行主题提取，归并。并不会对其结果进行分类等研究。
互联网上也有python 实现的文本话题检测项目，这些项目一般是学习算法过程，或处理小样本集，因此大多只是实现算法模型或只检测文本主题。

1.3课题内容及技术路线
课题主要是用Python将mysql中的邮件正文读取出来，利用gensim库实现LDA算法来检测邮件文本主题,并对其处理并储存,最后将mysql中的主题数据进行提取并进行聚类分析.
课题任务：
1.将数据库中的邮件文本进行读取，对每一封邮件进行主题提取，将提取结果进行处理并储存到数据库中。
2.将数据库中的主题提取的结果读取出来，进行聚类，经过处理得到分类结果。提取结果文件中的关键词作为类名结果。
3.将上一步中的结果制作成类名与类名词汇相对应的字典，用字典与数据库中的主题提取结果进行对比得到邮件分类结果。
实现该课题的大致流程：
1.manjaor(archliunx)下的python3.7作为基础环境。利用mysql-connector（一种python 的 mysql驱动）来连接数据库MYSQL。将Message表中的body字段，和sender字段从Mysql中读取得到邮件的所属人与内容。
2.将上一步读取出来的邮件内容进行作为文本进行文档清洗。首先利用英语文本特点，用句号对文本进行分句, 再利用NLTK（自然语言处理包）的tokenize 模块对文档先进分词，在分词过程中利用正则表达式去除标点，特殊符号。然后将分词结果中的单词与stop-words（停用词列表）中的单词进行对比然后去除除停用词。最后进行词干提取或词形还原，得到单词的一般形式。将上述结果进行整理并利用TF-LDF算法提取文本特征，转换成文本特征矢量。
3.利用gensim（LDA 算法实现第三方包）库中的LDA模块提取文档的主题,将所得主题进行输出，观察提取主题效果和结果格式。进一步将结果进行格式处理转换成一般字符串。然后在数据库建立新表保存结果，表中包含有sender字段，kw字段，weight字段，分别储存文本发送人，文档关键字，及关键字所占比重。
4.读取数据库中提取主题结果数据,对数据进行格式处理，在此时可以去除那些无意义单词。然后将数据转换成特征向量。最后用sklearn（python第三方包 数据挖掘和数据分析工具）中的K-means模块和Birch模块进行结果聚类。
5.将聚类结果进行聚类效果评估，并且进行可视化。然后将聚类结果保存得到文件中。将分类的单词做一个字典，用关键词来表明类别。


第二章技术原理
本项目中应用了LDA算法模型,聚类算法,以及分词与提取词干等许多技术,本章就这些技术进行一些技术基本原理介绍并且说明不同技术选用的优势与弱点..
2.1 数据清洗
本项目的数据是邮件文本数据,文本存在有URL链接,表情,特殊符号,俚语等.文本数据的有部分都是一些非传统意义上的文档，例如甚至一些文本基本有邮件地址,符号,和少量的文字组成.对于建模模型来说,它们中的这些无意义单词,符号等就是干扰数据。为了获得更好的数据分析结果，必须进行数据清洗。数据清洗要进行分句分词,移除Stop word,移除标点符号,表情符这种情况可以用NLTK的正则表达式来完成。
1.1.2分词处理
在进行LDA主题提取前,要将待检测得到文本制作成词袋.因从要将文本分割成词.首先根据空格拆分单词,英语的句子基本上就是由标点符号、空格和词构成，所以只需根据空格和标点符号将词语分割成数组即可。一般的英文分词可以利用空格和标点符号分割进行分词，也可用nltk进行分词。停止词，在英语中是stopword，英语文本文档中会有很多and，the，unless,等没有实际意义的连词,介词等.这些英文停用词词因为使用频率高,这类高频词会对基于词频的算分公式产生极大的干扰，而这些词几乎每篇文章都存在，如果文本处理要将这些词都处理的话，运算处理量会变得极大。所以文本处理时要构建一个与文本对应的停用词表,用于去除停用词.
文本分词可以通过正则表达式过滤一些符号,特殊字符,表情等.如下图所示：

图 2.1 正则表达式
1.1.3  提取词干
词干提取( Stemming),是英语的特殊处理，提取词干就是将一些词的共有词根提取出来的过程。比如说英文单词有事=时态变换的变形，如进行时-ing和过去式-ed的变形，但是在统计词的频率时，这些单词应该被当做同一个单词。这种词干提取即相关的词归类到同一个词干下一般能得到较好结果，词干提取不仅仅提取时态变换单词，还能提取相似的单词共有词根，如复数等
词形还原是不同于词干提取的一种方法,它能把一个经过变形的英语单词还原为一般形式而保留能表达完整单词语义的部分，而词干提取,是抽取词的词干或词根,在一定程度上不一定能清楚的表达单词原本的含义.词形还原和词干提取都可以把变形的单词形态,派生词或复合词统一简化成词干或单词原形.这两种方法都是文本处理中英语常用的单词规范方法.可以各自不同的背景情况下,达到特定的效果.
	实现词干提取和词形还原的一般都都是利用语言的特性，语法规则，甚至是字典映射来实现词干或词根的提取，词干提取方法较简单,词干提取的结果可能并不太理想，可能是不完整的、没有意义的词。甚至只是词的一部分。已将看不出单词的原意。词形还原在运算和算法上较复杂,不仅要对单词词形词缀进行分析,类似于词干提取中提取词根，还可以进行词性识别,以此来提高词性还原准确率.词形还原一般得到的结果都是有意义的单词，可以很好地应用到文本分析，聚类，自然语言中。
1.1.4 统计词频向量
Doc2Bow是Gensim库中的一个方法,可以实现Bow模型.它的基本思想忽略单词词序和语法等,将文本看做是一些词的集合.然后将文档转换成特征矢量。
基于频率的方法包括特征频率和文本频率。特征频率方法认为文本特征在文本集中体现出来词出现次数更多，可以用来过滤无关特征。文本频率用特征比例来衡量该特征的重要性。
​	TF方法可以有效降维。去除大量的低频特征，DF方法可以去除噪声特征，但如果某个特征仅存在于某一类文本中，该特征可有效地区分该类，然而根据DF方法它可能被作为噪声.TF-IDF 算法如下图所示:

图 2.2 TF-IDF 算法流程
2.2 聚类Kmeans Birch算法
聚类的就是通过一定的规则或标准来划分数据集。以便下一步进行数据分析，统计。聚类与分类不同，聚类不用提前标记训练集和监督。可以更好的进行数据划分。
常见的聚类分析方法有很多,如:层次法（Hierarchical Clustering）,如birch算法划分法（Partitive Clustering）如:kmeans算法.
2.2.1 birch算法介绍及特点
BIRCH的全称是利用层次方法的平衡迭代规约和聚类（Balanced Iterative Reducing and Clustering Using Hierarchies）[20]，是用层次方法来聚类和规约数据的,BIRCH只需要单遍扫描数据集就能进行聚类[21].
BIRCH算法利用了一个树结构来快速的聚类，这个数结构类似于平衡B+树，一般将它称之为聚类特征树(Clustering Feature Tree，简称CF Tree)[22]。这颗树的每一个节点是由若干个聚类特征(Clustering Feature，简称CF)组成,每个节点包括叶子节点都有若干个CF，而内部节点的CF有指向孩子节点的指针，所有的叶子节点用一个双向链表链接起来[11].birch CF树建立如下图所示:

图 2.6
BIRCH算法和kmeans算法不同，不用必须设置最终聚类数目K，如果k为NULL,最终的聚类数目就是birch算法中CF的数目。如果设置了K,birch 会进一步的聚类，直到聚类数目等于K。Birch 与kmeans相似的地方在于都可以进行大规模的数据处理，速率快，但birch可以在数据特征维度不大，分类类别较大的情况下表现的更好出色.　
BIRCH算法的主要优点在于：节约内存，在大规模数据训练情况下表现更明显。因为cf 树操作简单快速，所以聚类速度快，可以甄别噪音点。
　　BIRCH算法的主要缺点有：高维度数据集的聚类效果不如其他算法好。
更适用于数据集特征表现是凸的情况，否则聚类效果可能也达不到预期。
2.2.2 Kmeans 算法介绍及特点
K-Means算法的思想比较简单，是通过距离来划分类别。因此对于给定的数据集，按数据中的点之间的距离大小划分数据。以距离来衡量数据对象间相似性度量，如果数据集点与点的间的距离越小，那说明它们的相似性较高.这样就可以认为它们是同一个类簇.Kmeans尽量让簇内的数据聚集到一起，让簇与簇之间的距离更远。Kmeans算法如图:

图 2.4 kmeans 算法流程
K-Means优点：原理比较简单，比较好理解与实现，并且收敛速度快，节省时间。聚类效果较好，可以较好的用于大规模数据集聚类处理[24]。
　　K-Means的缺点有：K值的选取不好把握,K值的选取某种意义上决定聚类的结果和优劣。Kmeans算法对数据集中的异常数据和噪声比较敏感。

2.3 Lda 算法介绍及特点
LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机[25]。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。
2.3.1  LDA文本原理及生成过程
一篇文档，可以看成是一组有序的词排序而组成的序列,单从统计学角度考虑，文档的生成可以看成是随机概率生成的结果，每生成一个词汇便是一次概率作用结果，连续作用生成一篇文档[26]。因此,文本模模型取决于主题库中主题概率和词库中词的概率；
概率分布角度看，主题的产生服从一个概率分布, 即先验分布。对每个具体的主题，由该该主题对应的词库产生语料库的概率为p 故产生语料库的概率就是对每一个主题上产生语料库进行积分求和.先验概率有很多选择,多项式分布和狄利克雷分布是共轭分布，因此一个比较好的选择是采用狄利克雷分布[26]


图3.2 LDA原理图
LDA(Latent dirichlet allocation)]是三层贝叶斯主题模型，这种方法可以提取文本主题即发现隐藏语义topic而不用监督。模型主要是利用文本中词的分布特征来发现文本所含有的潜在主题。
LDA生成模型，实际上是默认的文本中每个词都是以某种概率选择了主题库的某个主题，而这个主题中以一定概率选择词库中的某个词语。在这个过程，文档到主题服从多项式分布，主题到词服从也多项式分布[28]。每篇文档代表了某些主题所组成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布[12]。基于这个LDA模型求解的每一篇文档的主题分布和每一个主题中词的分布一般有两种方法，第一种是基于Gibbs采样算法求解，第二种是基于变分推断EM算法求解。Gibbs采样算法求解如下图所示:



图 2.5



第三章 文本预处理
由于计算机并不能识别文本进行分析,所以要将文本形式进行转化向量以供计算机是捏计算.本章主要写的是对数据的处理和实例,如数据清洗方法,数据储存格式以及数据转换流程方式.
3.1数据集概述
本项目数据集是安然公司员工邮件及安然公司与其他人之间的通信邮件内容集合,共计25万条数据.数据库中共有四张表.如下图所示:

图 3.1数据库表
employeelist 表中主要存储安然公司员工信息,如,员工姓名,邮箱,身份信息等. recipientinfo表中主要是接收人的信息.主要是收件人邮箱地址,邮件编号等,message表中主要存储这邮件主要信息,如发件人,发送日期,发送的邮件主题,邮件编号,以及邮件文本主体.referenceinfo表中主要是邮件的参考信息.



图 3.2邮件内容样本


图 3.2邮件内容样本
在25万条数据中存在同一发送人,即同一邮箱..在这些数据中,body有短文本,也有较长文本.有些为转发内容,文本相同,或相似.所以要进行数据清洗.在body文本中,文本有俚语,URL链接,还有一些表达式,和停用词.本项目数据清洗主要是移除停用词如 are you if ….,及标点符号.

图 3.3 员工信息样本
在employeeelist表中,发现共145个安然员工.每个员工在数据库中存有姓与名,邮件地址,职位等.但大部分员工拥有多个邮箱.由于安然内部人员少,.因而有大量外部人员或公司与安然公司员工的通信.这部分邮件也代表着某些话题.以及安然员工个人关系圈子。
3.2 MySQL读取,存储
由于MySQL不能直接被所写的Python程序调用,需要支持Python的MySQL驱动来连接到MySQL。因此本项目利用mysql-connector 库来实现python对数据库的读写,插入,建表等操作。
3.2.1 数据库读取
Mid字段为邮件编号,sender字段为发送人,date字段是发送日期,subject 是邮件主题,body为正文文本数据.

图 3.4 邮件文本储存格式
3.2.2 数据库结果保存
数据结果保存在 KW_weight表,ID字段是邮件编号,sender字段是邮件发送人,kw1,kw2,kw3字段是邮件文本提取主题,weight1,weight2,weightt3字段是主题权重,如下图

图 3.5 结果存放格式

3.3 文本处理
文本处理要经过NLTK分句分词去除标点符号,去除无意义的停用词,并将文本转换成词袋模型,用TFLDF可提取文本特征.
3.3.1 NTLK介绍
NLTK，自然语言处理工具包，是NLP研究领域常用的一个Python库，这个开源项目，包含数据集、Python模块等,经常用于自然语言的处理.
3.3.2 分词提取词干
将数据库读出来的文本全部转换成小写格式,然后按空格把文档分割,分开的词组组成一个词汇列表.
tokenizer =RegexpTokenizer(r'\w+')
	#tokenizer =RegexpTokenizer(r'\w+|\$[\d\.]+|\S+')
利用RegexpTokenizer过滤去除噪音符号： ["\"","=","\\","/",":","-","(",")",",",".","\n"]等
stop_words = get_stop_words('en')	
stemmer = PorterStemmer()		# 提取词干
	lemmatizer = WordNetLemmatizer() #	词形还原
获取英文单词的停用词,与词汇列表中的词对比去除停用词.最后提取词干，把相近的词转提取词干或词性还原，比如把文档中的 use,used,using统一成use
最终程序如下图所示


图3.6 数据处理
程序中的body 是即将要处理的句子,首先要经过分词tokenizer.tokenizer()的处理,生成一个单词列表,将列表中单词于停用词列表进行比对去除停用词.然后声明一个新列表,将上一步处理过的数据进行词干提取或词形还原,最终将所有句子进行处理并添加到列表texts中.此时数据清洗完成.
3.4 文本转换
在数据清洗之后,文档的干扰字符将大大减少,同时,相等部分单词被还原成同一个词根.计算机本身是无法识别存储在词料库中的文本数据的,因此需要将词映射到相对应的数字上,这就是要建立数字和词的对应关系,即字典(dictionary). 字典包含词以及词在词典中对应的位置。 接着用doc2bow模块处理字典得到语料中每一篇文档对应的稀疏向量.
逆文档频率IDF（Inverse Document Frequency）是将最常见的词赋予最小的权重，较少见的词反而给予较大的权重。TF代表着一个词出现的频率;TF-IDF,就是词频（TF）与逆文档频率（IDF）相乘得到TF-IDF值.也就是说,词对文章的重要性与词的TF-IDF值成正比.代码如图:

图 3.7 向量转换代码
如下图,图中dict与corpus就对应着数字和词的对应关系dictionary和doc2bow模块处理字典处理过语料得到的结果向量,向量的每一个元素代表了一个单词在这篇文档中出现的次数.



图 3.8 字典与向量


第四章：话题检测分类的实现和分析
在词料经过处理后,进一步进行话题检测与聚类.在这过程中主要利用LDA算法模型,k-means聚类算法,birch聚类算法.本章就这些算法Python第三方包的实现进行参数说明和检测和聚类效果进行分析.
4.1 gensim主题提取
Gensim是一个Python第三方库，能从文档中自动提取文档主题,可以处理，非结构化的短文文本等。Gensim里面的算法，比如LSA,PLSA,LDA。这些算法是非监督的，只需要一个语料库。就可以从文档中提取主题。
4.1.1  LDA提取主题
LDaMode模块l用于提取文档的主题,它的参数影响着程序运行速度,空间,以及主题提取结果. 
lda = LdaModel(common_corpus, num_topics=50) 
关键参数说明:Corpus,文档向量流或稀疏形状矩阵;num_topics,要从训练语料库中提取的潜在主题的数量;pass 训练期间通过语料库的次数。
show_topics（num_topics = 10，num_words = 10，log = False）
这个方法是要获取所选主题的分布;num_topics	要返回的主题数;num_words为每个主题显示的单词数。这些将是最相关的单词（为每个主题分配最高概率）。
文档样本经过分词,去除停用词,特殊符号后生成一个二维列表.下列样本是一个较小的文本文档.经过处理后,可以看到文本此时只剩于一些主语,名词等.这里采用的是词形还原的词形规范方法.可以明显的看出单词的含义.如下图所示:

图 4.
文档进一步转换,利用LdaModel 模块中的TF-LDF来统计词频,并转换成向量.  L加载向量进行LDA算法提取样本主题,最终生成结果如下:

图 4.
结果中的单词是LDA提取主题所产生单词权重较高的部分.通过对这部分的解读,可以看出文档主要话题.结果中的数字便是主题在文档中所占有的权重.提取其中的单词及权重,和程序中获取到的发送人一同写入数据库中保存.如下图:

图 4.1 port-stem图
图4.1是程序经过port-stem算法提取词干最终产生结果,从图中可以看到,有部分单词已经没有实际意义了,不能通过结果看出词对应的主题具体是什么.但大部分单词的处理效果还是比较好的.能从3个词中联想大致看出文档主题.

图 4.2 词形还原处理
图4.2 是程序讲过词形还原处理,以求最大意义保留词汇愿意.从图中的结果可以看得出来,单词都有清晰的含义,能从单词轻易地看出主题大致内容.但是,这种方法在处理时,较词干提取更耗时间,最好还要标记词性,以求最好效果.
4.2 sklearn kmeans划分聚类
K-means算法通常有三个步骤。第一步选择初始质心，即初始的聚类中心
，一般是从数据集选择样本。初始化质心后，K-means在首先将每个数据样本点分配到离它最近的质心。然后根据现有的点进行聚类中心的计算，算法一直重复，算法结束条件可以是几乎没有数据被重新分配和聚类中心,即质心不在再发生变化.k-means 可以快速聚类而且获得一个较好结果.
参数说明:X	类似数组或稀疏矩阵;n_clusters	程序要形成的簇数即最终的类别数目;max_iter 程序运行的k-means算法的最大迭代次数。;label label [i]是第i个观察最接近的质心的代码或索引。
sklearn.cluster.k_means模块中的训练方法fit（X [，y，sample_weight]）	可以计算k均值聚类, fit_predict（X [，y，sample_weight]）	计算聚类中心并预测每个样本的聚类类别索引。
经过fit_predict()训练,可以得到一个分类标签列表,即图中的Predicting result,词料中的词都被赋予了标签,根据标签经过处理可以分类.
Intertia是样本距其最近的聚类中心的平方距离之和，用来评判分类的准确度，值越小越好 k-means的参数n_clusters可以通过这个值来评估如下图展示:

图4.3 kmeans标签图
程序对上述结果进一步处理,并对其可视化.方便观测聚类效果.经过对比发现,KMeans中的参数n_clusters的选取影响着程序聚类的效果,合适的值可以更直观的看出分类.数据数量也会影响聚类效果，数据量越多向量空间越大，越能体现出文本特征。将将数据库中的主题读取出来进行提取特征之前，可以过滤一些单词，如无法看出主题意义的，数字字符串等等， 此处的单词的处理也会影响聚类效果。如下图对比:




图 4.4 Kmeans 可视化图
在图4.4 中，左上角的图为未经过单词过滤处理，右下角的图是经过单词的过滤处理的。它们的聚类类别都是3类。右上角的图是未经过单词过滤处理，左下角的图是经过单词的过滤处理的。它们的聚类类别都是5类。可以清晰的发现，经过处理后的数据向量空间分布更有规律。大多会形成圆环。而没有经过单词过滤处理的的图，数据向量的点更混乱。增加数据量的处理，如图中左上角和右上角的图对比，数据量越大，点与点交错的更多，更乱。在用定量的数据进行聚类时，也可以达到一个较好的结果。
进一步整理结果，将分类标签与词料中的句子对应，并添加姓名，也就是邮件发送人。一起写入文件，如下图所示：
图 4.5 聚类结果图
本次实验是Kmeans聚类，参数设定一共聚类7类，分别对应着文件data2 文件中的result1 ,result12,result3 等等，对result0中的enron 关键词进行搜索，得到结果，即是图中选中部分。可以发现enron出现频次较高，说明有关这个主题的单词都被聚类到同一个文件下了，本次聚类效果较好。
4.3 sklearn birch层次聚类
Birch算法有两个重要参数，即阈值和分支因子。分支因子限制节点中子群集的数量，阈值限制输入样本与现有子群集之间的距离。
可以通过设置此聚类参数n_clusters。如果n_clusters设置为空，就会将CF tree 生成的类别数当做最终类别数目,如果n_clusters给定的数目较小,则要进一步聚类,最终收敛到n_clusters的值位置.birch函数说明如下图所示:

图 4.
参数说明:branching_factor,每个节点中的最大CF子集群数;n_clusters	最终聚类步骤之后的聚类数，它将叶子中的子聚类视为新样本;compute_labels是否为每个拟合计算标签;labels_分配给输入数据的标签数组.
class sklearn.cluster.Birch的方法主要用到的是fit_predict（X，y =无）,功能实现的是在X上执行群集并返回群集标签。
Birch 经过fit_predict()训练后,如同kmeans一般,可以得到一个分类矩阵.矩阵规模取决于语料库的大小.聚类标签总数目就是n_clusters参数的赋值.轮廓系数结合了聚类的凝聚度和分离度，用于评估聚类的效果,该值处于-1~1之间，值越大，表示聚类效果越好[13].矩阵中的数字便是聚类标签,如下图:

图 4.6 birch结果标签
图中的矩阵规模是（76，464），也就是说共有76个对象参与聚类，也就是76个文本。464是说聚类的文本向量是464维的。将上述birch算法聚类所得数据进行可视化,图中不同颜色表明不同的类别,图中的红线就是聚类的轮廓系数,本样本将语料分为12个类,轮廓系数大于0.4.如下图:

经过处理可以把数据样本根据分类类别分别写入文件,这样就可以很直观的看到分类数据聚类效果.将文档LDA 提取的关键词于聚类主题对比,就能发现文档所属分类,将邮箱发送人作为文档所有人进行分类.并写入文本保存.文本文件如图所示:

图 4.4

Bich 聚类一共聚类12类,分别对应图中的result1.txt, result2.txt等等文件.对分类结果关键字搜索,例如 ‘enron’,可以发现enron这个词在文件结果出先频次较高.对应图中选中部分.在文件保存时,将发送人作为邮件所属人写入文件.如图4.4中文本所属邮箱字段对应的就是 发送人的邮箱地址.






















第五章：问题总结与展望
在实现整个项目期间,遇到一些程序问题或效果不理想的情况,如运行报错,参数不合理造成聚类结果不好.本章就这些问题进行总结并给出解决方案,并说对解决方案进行效果分析.
5.1 程序运行优化问题
原因:数据库中邮件数据量过大,并且有些邮件属于大文本,加载训练时,比较耗用处理时间内存.数据库中存有的25万条数据要先将过LDA算法进行主题提取,并将提取的结果插入数据库.在程序训练和插入数据这两个处理阶段耗费时间最多.
解决方案:
1.控制LDA主题提取训练轮数,即pass参数,在不太影响程序结果前提下,减少文本训练时间和占有内存.
2.使用多进程来加快处理程序,充分使用硬件性能,减少运行时间.
采用上述方案效果如下图所示:

图5.1 多线程图
程序由于采用进程池来开启多线程,四核电脑CPU占用率可以达到100%,可以充分利用CPU计算性能.进程池中开启8个进程,对应如图中的 pid 16527,16529等8个进程.
经过参数的调整和多进程技术,在内存保障充足的条件下,程序运行时间大致可以减少4倍.


5.2 聚类格式处理问题
原因:gensim和sklearn库处理算法相同,但结果数据格式表示不同,sklearn 包的格式同
Numpy,scipy等科学计算包是相匹配的.能更好的表示向量中的稀疏矩阵与密集矩阵.
Gensim库中的数据格式一般使用稀疏矩阵表示的.如果将gensim处理过的数据直接进行Sklearn包的处理时,一般都会报错,如下图所示:

图5.4
解决方案:可以将gensim包中中的corpus语料格式转化成csr_matrix.通过scipy模块将数据转换成为sklearn可训练的格式.也可以统一使用gensim或sklearn的数据处理库,避免数据格式不匹配.

5.3 聚类大维度问题
原因:将文本主题提取内容根据邮件发送人整理成字典生成语料库,并进一步处理转换特征向量准备聚类时,因为邮件较多,语料库中主题关键词也会较多.这时就可能就会维度过高而程序报错,或造成内存耗尽而程序僵死.无法接着完成聚类预测,生成标签等下一步内容.
解决方案:所提取主题文本过大时,可对文本生成的向量进项降维,减少内存占用.可以考虑数据降维,数据降维时,在确保数据特征明显的情况下,减少维度.不仅可以减小程序运行压力,还可以更好的对数据进行聚类.但是数据维度并不是过大时,将原始矩阵降维，所得的效果反而没有不降维的好.降维的维度数目要根据程序文本所产生的效果多次验证调整.本项目采用的是PCA算法进行降维.如下图所示:


图5.6
PCA通过线性变换将原来数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维,sklearn中实现只用调用PCA.fit_transform()方法得到.
在读取数据后在保障数据相同,数据格式处理相同的情况下,采用上述方案效果对比如图所示:

图 5.6

如图5.6 所示,图中左侧的图是经过PCA降维处理的,右侧的数据没有经过降维,从图中可以看出来.经过数据降维后左侧的平均轮廓系数要高于右侧的平均轮廓系数,也就是图中虚线的位置.从总的方面讲,左侧图的的数据分类效果要好于右侧没有经过降维处理的数据.


















致谢
时光飞逝,转眼间四年的大学时光已经静静的过去.回想起刚踏入校园的我,怀着激动地心情,开始人生中的青年时光旅程 .在四年校园生活里,北方民族大学提供了有优美的学习环境,愉快的氛围.期间发生许许多多的故事,它们点缀着我的生活,我的人生.留下令人难忘的记忆.对于即将踏出校园的我,真心感谢关心过我的老师,帮助过我的同学,还有那总是要为我们提供最好环境的学校.希望母校在以后的日子里发展的更好,提供更好的学习环境与氛围.培养更多的学生.
在毕业前,毕业设计是每个人都要完成学习任务的最后考验.在这四年中最重要的学习任务,而富有挑战的实践中,我的指导老师于千诚老师给了我很大的帮助.在我一次次请教中,于千诚老师总是耐心的给我讲解毕业设计问题.在老师的督促指导下,我完成了大学生涯中的最后一张试卷.在这过程中,同学们也给了我很大的帮助,在一次次老师讲解完成后,我与同学们总是会讨论设计的最佳方案,反复实现设想来实现对比以求最优解.平时的学习生活中,老师们同样也给了我很多帮助,同学们也乐于互相交流,指出对方理解的误区.在这种环境中,我收获了的不只是知识,还有友情等.对于辛勤工作的老师们,互相帮助的同学们在此感谢它们.
感谢父母多年无私奉献支持,有了你们的支持,我来能无忧无虑的求学.在异地求学,这些年很少陪伴父母,你们辛苦了!感谢你们多年的悉心指导与帮助,你们的孩子已将长大,也经可以肩负家庭的责任.今后,你们可以更轻松的享受生活了.
感谢四年中帮助过我的同学,老师,朋友们.正是有你们的鼓励与支持,我才有向前奋斗的信心与动力.最后，再次感谢我的论文导师于千诚老师在我论文和毕设创作过程中的耐心指导,还有同学们的关心与帮助.感谢!
参考文献
[1]聂晶.Python在大数据挖掘和分析中的应用优势[J].广西民族大学学报(自然科学版),2018,24(01):76- 79.
[1] 胡吉明,陈果.基于动态LDA主题模型的内容主题挖掘与演化[J].图书情报工
[2] www.jianshu.com从LSA/LSI潜在语义索引到LDA狄利克雷分布[EB/OL] <https://www.jianshu.com/p/283691c40fa8>
[3] www. blog.csdn.net LSA/PLSA[EB/OL]
<https://blog.csdn.net/zhoubl668/article/details/7881318>
[4] www.sohu.com一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模[EB/OL]
<http://www.sohu.com/a/234584362_129720>
[5] www.cnblogs.com简述LDA主题模型[EB?OL]
http://www.cnblogs.com/fengsser/p/5836677.html
[6] 石晶, 李万龙. 基于LDA模型的主题词抽取方法[J]. 计算机工程, 2010, 36(19):81-83.
[10] 基于FP-Growth算法的文本关联分析 郝枫 《电脑开发与应用》2008年
[11] [https://www.cnblogs.com/pinard/p/6179132.html
[12] https://blog.csdn.net/wind_blast/article/details/53815757
[13] https://www.cnblogs.com/bourneli/p/3645049.html
[14] [http://www.doc88.com/p-5919647224522.html
[21] 社会网络特征分析与社团结构挖掘 刘瑶 《电子科技大学》2013年 
[20]http://www.cnblogs.com/pinard/p/6179132.html
[22] 空间试验及仿真可视化通用支撑平台的研究与实现 罗丹 《国防科学技术大学》2005年
[24] Kmeans++、Birch和KNN四种聚类算法对二维坐标点的聚_CSDN博客 《互联网资源库(https://blog.csdn.net/zh)》
[25] 基于非结构化数据的多类型网络构建研究 吴广程 《北京邮电大学》2010年
[26]  基于赋权有向图的邮件网络亲缘关系研究与实现 项学涛 《国际关系学院》2015年
[28] 大型复杂网络中的社区结构发现算法 胡健 董跃华 杨炳儒 《计算机工程》2008年

